{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Project Work Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Brief Overview of Work Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image                  \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files       \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize the Data\n",
    "The data needs to be organized so that it can be easily input into different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dog/Human Data\n",
    "This will be used to train the Dog/Human detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Collect all Dog Data\n",
    "dog_data = np.expand_dims(np.array(glob('dogImages/*/*/*')), axis=1)\n",
    "np.random.shuffle(dog_data)\n",
    "\n",
    "# Collect all Human Data\n",
    "human_data = np.expand_dims(np.array(glob('lfw/*/*')), axis=1)\n",
    "np.random.shuffle(human_data)\n",
    "\n",
    "# We want equal data from dog and human categories so we only use some of the human data\n",
    "human_data = human_data[:dog_data.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So the data has been put in the numpy arrays and shuffled. The next step is to:\n",
    "<ol>\n",
    "    <li>Assign Labels</li>\n",
    "    <li>Combine the Arrays</li>\n",
    "    <li>Reshuffle the Data</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assign labels to data; 1 for Human 0, for Dog\n",
    "human_labels = np.ones(human_data.shape, dtype=np.float32)\n",
    "dog_labels = np.zeros(dog_data.shape, dtype=np.float32)\n",
    "\n",
    "# Combine labels with respective data, combine all data into one array\n",
    "labeled_human_data = np.concatenate((human_data, human_labels), axis=1)\n",
    "labeled_dog_data = np.concatenate((dog_data, dog_labels), axis=1)\n",
    "detector_data = np.concatenate((labeled_human_data, labeled_dog_data), axis=0)\n",
    "\n",
    "# Shuffled data for detector\n",
    "np.random.shuffle(detector_data)\n",
    "\n",
    "# Split into input and labels\n",
    "detector_inputs = detector_data[:,0]\n",
    "human_det_labels = detector_data[:,1].astype(np.float32)\n",
    "\n",
    "# Flip labels for dog detector\n",
    "dog_det_labels = 1 - human_det_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next cell shows that the first 10 entries and the labels make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: dogImages\\train\\069.French_bulldog\\French_bulldog_04767.jpg      Human Label: 0.0      Dog Label: 1.0\n",
      "Filename: dogImages\\train\\130.Welsh_springer_spaniel\\Welsh_springer_spaniel_08218.jpg      Human Label: 0.0      Dog Label: 1.0\n",
      "Filename: lfw\\Elizabeth_Hurley\\Elizabeth_Hurley_0005.jpg      Human Label: 1.0      Dog Label: 0.0\n",
      "Filename: dogImages\\train\\090.Italian_greyhound\\Italian_greyhound_06150.jpg      Human Label: 0.0      Dog Label: 1.0\n",
      "Filename: dogImages\\train\\086.Irish_setter\\Irish_setter_05870.jpg      Human Label: 0.0      Dog Label: 1.0\n",
      "Filename: dogImages\\train\\107.Norfolk_terrier\\Norfolk_terrier_07066.jpg      Human Label: 0.0      Dog Label: 1.0\n",
      "Filename: lfw\\Abdullatif_Sener\\Abdullatif_Sener_0002.jpg      Human Label: 1.0      Dog Label: 0.0\n",
      "Filename: lfw\\Osmond_Smith\\Osmond_Smith_0001.jpg      Human Label: 1.0      Dog Label: 0.0\n",
      "Filename: lfw\\Pervez_Musharraf\\Pervez_Musharraf_0012.jpg      Human Label: 1.0      Dog Label: 0.0\n",
      "Filename: lfw\\Walter_Mondale\\Walter_Mondale_0002.jpg      Human Label: 1.0      Dog Label: 0.0\n"
     ]
    }
   ],
   "source": [
    "for item in zip(detector_inputs[:10], human_det_labels, dog_det_labels):\n",
    "    print('Filename:', item[0], '     Human Label:', item[1], '     Dog Label:', item[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, the data needs to be split into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16702,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we want 2000 validation and 1000 test examples\n",
    "test_index = (0, 1000)\n",
    "validation_index = (1000, 3000)\n",
    "train_index = (3000, 16701)\n",
    "\n",
    "def get_data(inputs, labels, indices):\n",
    "    '''\n",
    "        function desc.\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        Returns:\n",
    "    '''\n",
    "    input_data = inputs[indices[0]: indices[1]]\n",
    "    data_labels = labels[indices[0]: indices[1]]\n",
    "    return input_data, data_labels\n",
    "\n",
    "hd_train_items, hd_train_labels = get_data(detector_inputs, human_det_labels, train_index)\n",
    "hd_valid_items, hd_valid_labels = get_data(detector_inputs, human_det_labels, validation_index)\n",
    "hd_test_items, hd_test_labels = get_data(detector_inputs, human_det_labels, test_index)\n",
    "\n",
    "dd_train_items, dd_train_labels = get_data(detector_inputs, dog_det_labels, train_index)\n",
    "dd_valid_items, dd_test_labels = get_data(detector_inputs, dog_det_labels, validation_index)\n",
    "dd_test_items, dd_test_labels = get_data(detector_inputs, dog_det_labels, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dogImages\\\\train\\\\099.Lhasa_apso\\\\Lhasa_apso_06643.jpg', 1.0)\n",
      "('dogImages\\\\train\\\\012.Australian_shepherd\\\\Australian_shepherd_00825.jpg', 1.0)\n",
      "('dogImages\\\\test\\\\070.German_pinscher\\\\German_pinscher_04861.jpg', 1.0)\n",
      "('dogImages\\\\train\\\\046.Cavalier_king_charles_spaniel\\\\Cavalier_king_charles_spaniel_03328.jpg', 1.0)\n",
      "('dogImages\\\\train\\\\075.Glen_of_imaal_terrier\\\\Glen_of_imaal_terrier_05141.jpg', 1.0)\n",
      "('dogImages\\\\valid\\\\101.Maltese\\\\Maltese_06737.jpg', 1.0)\n",
      "('lfw\\\\Nestor_Kirchner\\\\Nestor_Kirchner_0035.jpg', 0.0)\n",
      "('lfw\\\\Richard_Gephardt\\\\Richard_Gephardt_0001.jpg', 0.0)\n",
      "('dogImages\\\\train\\\\024.Bichon_frise\\\\Bichon_frise_01772.jpg', 1.0)\n",
      "('lfw\\\\Heidi_Fleiss\\\\Heidi_Fleiss_0003.jpg', 0.0)\n"
     ]
    }
   ],
   "source": [
    "for item in zip(dd_train_items[:10], dd_train_labels[:10]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dog Breed Data\n",
    "This will be used to train the breed classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    '''\n",
    "        function desc.\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        Returns:\n",
    "    '''\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "breed_train_files, breed_train_labels = load_dataset('dogImages/train')\n",
    "breed_valid_files, breed_valid_labels = load_dataset('dogImages/valid')\n",
    "breed_test_files, breed_test_labels = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert files to tensors\n",
    "The data needs to be converted to tensors and formatted to be input into pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hd_train_items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-696c95519339>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# human/dog detector data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhd_train_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhd_train_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mhd_valid_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhd_valid_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhd_test_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhd_test_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hd_train_items' is not defined"
     ]
    }
   ],
   "source": [
    "# human/dog detector data\n",
    "hd_train_tensors = paths_to_tensor(hd_train_items).astype('float32')\n",
    "hd_valid_tensors = paths_to_tensor(hd_valid_items).astype('float32')\n",
    "hd_test_tensors = paths_to_tensor(hd_test_items).astype('float32')\n",
    "\n",
    "dd_train_tensors = paths_to_tensor(dd_train_items).astype('float32')\n",
    "dd_valid_tensors = paths_to_tensor(dd_valid_items).astype('float32')\n",
    "dd_test_tensors = paths_to_tensor(dd_test_items).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6680/6680 [00:54<00:00, 123.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 835/835 [00:06<00:00, 134.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 836/836 [00:06<00:00, 138.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# breed detector data\n",
    "breed_train_tensors = paths_to_tensor(breed_train_files).astype('float32')/255\n",
    "breed_valid_tensors = paths_to_tensor(breed_valid_files).astype('float32')/255\n",
    "breed_test_tensors = paths_to_tensor(breed_test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo normalization for other preprocessing\n",
    "breed_train_tensors *= 255\n",
    "breed_valid_tensors *= 255\n",
    "breed_test_tensors *= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data for Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def imagenet_prep(image_tensors):\n",
    "    '''\n",
    "        Takes the image tensors and processes them for the models trained on ImageNet.\n",
    "        \n",
    "        Inputs: Image tensor of shape (images, width, height, colorchannels)\n",
    "        \n",
    "        Returns: Tensor of same shape with color channels flipped and values centered around zero for each color channel.\n",
    "    '''\n",
    "    # imagenet averages for RGB\n",
    "    image_net_mean = np.array([103.939,116.779,123.68])\n",
    "    image_tensors -= image_net_mean\n",
    "    \n",
    "    # Flip color channels\n",
    "    image_tensors = image_tensors[:, :, :, ::-1]\n",
    "    return image_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def unprocess(image_tensors, mode='caffe'):\n",
    "    '''\n",
    "        Unprocesses the preprocessing step for imagenet.\n",
    "    '''\n",
    "    if mode == 'caffe':\n",
    "        # Flip color channels\n",
    "        image_tensors = image_tensors[:, :, :, ::-1]\n",
    "\n",
    "        # Add RBG mean back to images\n",
    "        image_net_mean = np.array([103.939,116.779,123.68])\n",
    "        image_tensors += image_net_mean\n",
    "        image_tensors = np.clip(image_tensors, 0, 256)\n",
    "        return image_tensors\n",
    "    else:\n",
    "        image_tensors = np.clip((image_tensors + 1)*127.5, 0, 255)\n",
    "        return image_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Generator for Augmented Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = ImageDataGenerator(rescale=1.,\n",
    "                                        rotation_range=15.0,\n",
    "                                        width_shift_range=0.1,\n",
    "                                        height_shift_range=0.1,\n",
    "                                        shear_range=0.1,\n",
    "                                        zoom_range=0.1,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=False,\n",
    "                                        fill_mode=\"reflect\")\n",
    "\n",
    "validation_generator = ImageDataGenerator(rescale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_data = training_generator.flow(breed_train_tensors, breed_train_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_data = training_generator.flow(breed_train_tensors, breed_train_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Human/Dog Detector\n",
    "Build a model that predicts if there is a dog, human, both or neither in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Human Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_inputs, train_hlabels = get_data(detector_inputs, human_det_labels, train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13701/13701 [01:09<00:00, 196.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_inputs = paths_to_tensor(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mixed10/concat:0' shape=(?, ?, ?, 2048) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Dog Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Object Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Split Image for Images Containing Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breed Classifier for Dogs\n",
    "Build a model to predict the dog breed, optimized for pictures of dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### CNN from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reminder that the data arrays are named:\n",
    "<ul>\n",
    "    <li>breed_train_aug, breed_labels_aug</li>\n",
    "    <li>breed_valid_pp, breed_valid_labels</li>\n",
    "    <li>breed_test_pp, breed_test_labels</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               34181     \n",
      "=================================================================\n",
      "Total params: 1,013,701\n",
      "Trainable params: 1,013,189\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Conv layer1\n",
    "model.add(Conv2D(32, 3, strides=(1,1), padding='same', activation='relu', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "# Conv layer2\n",
    "model.add(Conv2D(64, 3, strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "# Conv layer3\n",
    "model.add(Conv2D(128, 3, strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "# Conv layer4\n",
    "model.add(Conv2D(256, 3, strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "# Conv layer5\n",
    "model.add(Conv2D(256, 3, strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides= 2, padding='same'))\n",
    "\n",
    "#Flatten Layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Fully Connected Layer 2\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 62s - loss: 4.7962 - acc: 0.0170 - val_loss: 4.8083 - val_acc: 0.0156\n",
      "Epoch 2/10\n",
      " - 63s - loss: 4.5329 - acc: 0.0362 - val_loss: 4.8776 - val_acc: 0.0192\n",
      "Epoch 3/10\n",
      " - 64s - loss: 4.3268 - acc: 0.0527 - val_loss: 5.1107 - val_acc: 0.0240\n",
      "Epoch 4/10\n",
      " - 61s - loss: 4.1206 - acc: 0.0765 - val_loss: 4.5218 - val_acc: 0.0563\n",
      "Epoch 5/10\n",
      " - 57s - loss: 3.8856 - acc: 0.1103 - val_loss: 4.3029 - val_acc: 0.0802\n",
      "Epoch 6/10\n",
      " - 57s - loss: 3.6471 - acc: 0.1379 - val_loss: 3.8123 - val_acc: 0.1473\n",
      "Epoch 7/10\n",
      " - 57s - loss: 3.4408 - acc: 0.1789 - val_loss: 3.9841 - val_acc: 0.1162\n",
      "Epoch 8/10\n",
      " - 57s - loss: 3.2625 - acc: 0.2068 - val_loss: 3.5572 - val_acc: 0.1653\n",
      "Epoch 9/10\n",
      " - 60s - loss: 3.0460 - acc: 0.2469 - val_loss: 3.3618 - val_acc: 0.2072\n",
      "Epoch 10/10\n",
      " - 60s - loss: 2.8351 - acc: 0.2874 - val_loss: 3.2156 - val_acc: 0.2156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x245134f80f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch_da.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "model.fit_generator(training_data, steps_per_epoch=len(breed_train_tensors)//32, epochs=epochs,\n",
    "                    callbacks=[checkpointer], verbose=2, \n",
    "                    validation_data=(breed_valid_tensors, breed_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "inception = InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bottleneck = inception.predict(breed_train_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_valid = inception.predict(breed_valid_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Define your architecture.\n",
    "\n",
    "inception_model = Sequential()\n",
    "inception_model.add(GlobalAveragePooling2D(input_shape=new_bottleneck.shape[1:]))\n",
    "inception_model.add(BatchNormalization())\n",
    "\n",
    "inception_model.add(Dense(500, activation='relu'))\n",
    "inception_model.add(BatchNormalization())\n",
    "inception_model.add(Dropout(.2))\n",
    "\n",
    "inception_model.add(Dense(500, activation='relu'))\n",
    "inception_model.add(BatchNormalization())\n",
    "inception_model.add(Dropout(.2))\n",
    "\n",
    "inception_model.add(Dense(133, activation='softmax'))\n",
    "inception_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 1.3206 - acc: 0.6732 - val_loss: 0.6862 - val_acc: 0.8132\n",
      "Epoch 2/50\n",
      " - 4s - loss: 0.5820 - acc: 0.8229 - val_loss: 0.6441 - val_acc: 0.8383\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.4117 - acc: 0.8675 - val_loss: 0.6631 - val_acc: 0.8216\n",
      "Epoch 4/50\n",
      " - 4s - loss: 0.3132 - acc: 0.8990 - val_loss: 0.7470 - val_acc: 0.8228\n",
      "Epoch 5/50\n",
      " - 4s - loss: 0.2646 - acc: 0.9141 - val_loss: 0.6859 - val_acc: 0.8311\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.2162 - acc: 0.9290 - val_loss: 0.7047 - val_acc: 0.8323\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.1713 - acc: 0.9458 - val_loss: 0.7077 - val_acc: 0.8299\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e5832dc752cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m inception_model.fit(new_bottleneck, breed_train_labels, \n\u001b[0;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbreed_valid_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m           epochs=50, batch_size=32, callbacks=[checkpointer], verbose=2)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.inception.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "inception_model.fit(new_bottleneck, breed_train_labels, \n",
    "          validation_data=(new_valid, breed_valid_labels),\n",
    "          epochs=50, batch_size=32, callbacks=[checkpointer], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_training = base_model.predict(breed_train_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_training = base_model.predict(breed_train_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable=False\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x284d7152198>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x284d2f4a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pre_training[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x284cf34c128>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwIHCCBwo7L9W0e4+7Q4ptQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x284ceb94eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(post_training[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "model2 = GlobalAveragePooling2D()(x)\n",
    "model2 = BatchNormalization()(model2)\n",
    "\n",
    "model2 = Dense(500, activation='relu')(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Dropout(.2)(model2)\n",
    "\n",
    "model2 = Dense(500, activation='relu')(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Dropout(.2)(model2)\n",
    "\n",
    "model2 = Dense(133, activation='softmax')(model2)\n",
    "model = Model(inputs=base_model.input, outputs=model2)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[311:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      " - 160s - loss: 2.5018 - acc: 0.4238 - val_loss: 1.3210 - val_acc: 0.6323\n",
      "Epoch 2/10\n",
      " - 34s - loss: 1.2753 - acc: 0.6463 - val_loss: 1.4657 - val_acc: 0.6192\n",
      "Epoch 3/10\n",
      " - 34s - loss: 0.9326 - acc: 0.7320 - val_loss: 1.4765 - val_acc: 0.6180\n",
      "Epoch 4/10\n",
      " - 34s - loss: 0.7156 - acc: 0.7832 - val_loss: 1.5489 - val_acc: 0.6036\n",
      "Epoch 5/10\n",
      " - 35s - loss: 0.5622 - acc: 0.8326 - val_loss: 1.6213 - val_acc: 0.6024\n",
      "Epoch 6/10\n",
      " - 34s - loss: 0.4524 - acc: 0.8614 - val_loss: 1.6362 - val_acc: 0.5976\n",
      "Epoch 7/10\n",
      " - 34s - loss: 0.4301 - acc: 0.8693 - val_loss: 1.6778 - val_acc: 0.6299\n",
      "Epoch 8/10\n",
      " - 34s - loss: 0.3752 - acc: 0.8814 - val_loss: 1.7161 - val_acc: 0.6120\n",
      "Epoch 9/10\n",
      " - 34s - loss: 0.3275 - acc: 0.8949 - val_loss: 1.7523 - val_acc: 0.6287\n",
      "Epoch 10/10\n",
      " - 35s - loss: 0.2964 - acc: 0.9073 - val_loss: 1.7972 - val_acc: 0.6132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x284d17c5f98>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.inception.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "model.fit(breed_train_tensors, breed_train_labels, \n",
    "          validation_data=(breed_valid_tensors, breed_valid_labels),\n",
    "          epochs=10, batch_size=32, callbacks=[checkpointer], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(include_top=False, weights='imagenet')\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model1 = vgg16.output\n",
    "model1 = GlobalAveragePooling2D()(model1)\n",
    "model1 = BatchNormalization()(model1)\n",
    "\n",
    "model1 = Dense(512, activation='relu')(model1)\n",
    "model1 = BatchNormalization()(model1)\n",
    "model1 = Dropout(.5)(model1)\n",
    "\n",
    "model1 = Dense(512, activation='relu')(model1)\n",
    "model1 = BatchNormalization()(model1)\n",
    "model1 = Dropout(.5)(model1)\n",
    "\n",
    "model1 = Dense(133, activation='softmax')(model1)\n",
    "vgg_model = Model(inputs=vgg16.input, outputs=model1)\n",
    "\n",
    "vgg_model.compile(optimizer=Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 72s - loss: 0.3303 - acc: 0.8923 - val_loss: 0.7709 - val_acc: 0.7904\n",
      "Epoch 2/10\n",
      " - 69s - loss: 0.3195 - acc: 0.8940 - val_loss: 0.7700 - val_acc: 0.7844\n",
      "Epoch 3/10\n",
      " - 68s - loss: 0.2802 - acc: 0.9075 - val_loss: 0.7430 - val_acc: 0.7916\n",
      "Epoch 4/10\n",
      " - 70s - loss: 0.2758 - acc: 0.9061 - val_loss: 0.8023 - val_acc: 0.7725\n",
      "Epoch 5/10\n",
      " - 69s - loss: 0.2471 - acc: 0.9219 - val_loss: 0.7432 - val_acc: 0.7832\n",
      "Epoch 6/10\n",
      " - 67s - loss: 0.2367 - acc: 0.9225 - val_loss: 0.7990 - val_acc: 0.7904\n",
      "Epoch 7/10\n",
      " - 67s - loss: 0.2062 - acc: 0.9335 - val_loss: 0.8464 - val_acc: 0.7784\n",
      "Epoch 8/10\n",
      " - 66s - loss: 0.2188 - acc: 0.9309 - val_loss: 0.7979 - val_acc: 0.7701\n",
      "Epoch 9/10\n",
      " - 72s - loss: 0.2058 - acc: 0.9329 - val_loss: 0.7578 - val_acc: 0.8012\n",
      "Epoch 10/10\n",
      " - 67s - loss: 0.1978 - acc: 0.9379 - val_loss: 0.7533 - val_acc: 0.7976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ffb15f0e80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "epochs = 10\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.vgg16.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "vgg_model.fit_generator(training_data, steps_per_epoch=len(breed_train_pp)//32, epochs=epochs,\n",
    "                    callbacks=[checkpointer], verbose=2, \n",
    "                    validation_data=(breed_valid_pp, breed_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### FineTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for layer in vgg_model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "vgg_model.compile(optimizer=Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 77s - loss: 0.7624 - acc: 0.7605 - val_loss: 0.8992 - val_acc: 0.7461\n",
      "Epoch 2/10\n",
      " - 70s - loss: 0.6548 - acc: 0.7928 - val_loss: 0.8947 - val_acc: 0.7353\n",
      "Epoch 3/10\n",
      " - 67s - loss: 0.5556 - acc: 0.8181 - val_loss: 1.0812 - val_acc: 0.7186\n",
      "Epoch 4/10\n",
      " - 67s - loss: 0.5046 - acc: 0.8344 - val_loss: 0.7735 - val_acc: 0.7760\n",
      "Epoch 5/10\n",
      " - 67s - loss: 0.4419 - acc: 0.8578 - val_loss: 0.8779 - val_acc: 0.7665\n",
      "Epoch 6/10\n",
      " - 67s - loss: 0.3951 - acc: 0.8692 - val_loss: 0.7796 - val_acc: 0.7856\n",
      "Epoch 7/10\n",
      " - 70s - loss: 0.3902 - acc: 0.8722 - val_loss: 0.8062 - val_acc: 0.7892\n",
      "Epoch 8/10\n",
      " - 66s - loss: 0.3354 - acc: 0.8842 - val_loss: 0.7984 - val_acc: 0.7713\n",
      "Epoch 9/10\n",
      " - 65s - loss: 0.3349 - acc: 0.8921 - val_loss: 0.7373 - val_acc: 0.7916\n",
      "Epoch 10/10\n",
      " - 65s - loss: 0.2884 - acc: 0.9061 - val_loss: 0.7662 - val_acc: 0.7868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f932059780>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.vgg16.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "vgg_model.fit_generator(training_data, steps_per_epoch=len(breed_train_pp)//32, epochs=epochs,\n",
    "                    callbacks=[checkpointer], verbose=2, \n",
    "                    validation_data=(breed_valid_pp, breed_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Several Models\n",
    "Use bottleneck features from several pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inception = InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model2 = inception.output\n",
    "model2 = GlobalAveragePooling2D()(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "\n",
    "model2 = Dense(500, activation='relu')(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Dropout(.8)(model2)\n",
    "\n",
    "model2 = Dense(133, activation='softmax')(model2)\n",
    "inception_model = Model(inputs=inception.input, outputs=model2)\n",
    "\n",
    "inception_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      " - 56s - loss: 4.4143 - acc: 0.2192 - val_loss: 1.4034 - val_acc: 0.6431\n",
      "Epoch 2/10\n",
      " - 34s - loss: 2.4806 - acc: 0.4289 - val_loss: 1.4389 - val_acc: 0.6371\n",
      "Epoch 3/10\n",
      " - 34s - loss: 2.0109 - acc: 0.5066 - val_loss: 1.3687 - val_acc: 0.6383\n",
      "Epoch 4/10\n",
      " - 34s - loss: 1.7714 - acc: 0.5530 - val_loss: 1.3314 - val_acc: 0.6455\n",
      "Epoch 5/10\n",
      " - 34s - loss: 1.6078 - acc: 0.5784 - val_loss: 1.3116 - val_acc: 0.6527\n",
      "Epoch 6/10\n",
      " - 35s - loss: 1.4510 - acc: 0.6118 - val_loss: 1.2775 - val_acc: 0.6539\n",
      "Epoch 7/10\n",
      " - 34s - loss: 1.3563 - acc: 0.6253 - val_loss: 1.2732 - val_acc: 0.6635\n",
      "Epoch 8/10\n",
      " - 34s - loss: 1.2416 - acc: 0.6522 - val_loss: 1.2947 - val_acc: 0.6551\n",
      "Epoch 9/10\n",
      " - 34s - loss: 1.2005 - acc: 0.6692 - val_loss: 1.2596 - val_acc: 0.6778\n",
      "Epoch 10/10\n",
      " - 34s - loss: 1.1542 - acc: 0.6728 - val_loss: 1.2801 - val_acc: 0.6659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118f41518d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.inception.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "inception_model.fit(inception_train, breed_train_labels, batch_size=32, epochs=epochs,\n",
    "                    callbacks=[checkpointer], verbose=2, \n",
    "                    validation_data=(inception_valid, breed_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import Resnet\n",
    "\n",
    "inception = InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model2 = inception.output\n",
    "model2 = GlobalAveragePooling2D()(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "\n",
    "model2 = Dense(1024, activation='relu')(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Dropout(.5)(model2)\n",
    "\n",
    "model2 = Dense(1024, activation='relu')(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Dropout(.5)(model2)\n",
    "\n",
    "model2 = Dense(133, activation='softmax')(model2)\n",
    "inception_model = Model(inputs=inception.input, outputs=model2)\n",
    "\n",
    "inception_model.compile(optimizer=Adam(lr=.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Organize Models into an Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Breed Classifier for Humans\n",
    "Build a model that predicts a breed that looks similar to the human in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Face Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Breed Classifier for Dog Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test Model on Human Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Face Matching\n",
    "Find a dog in the data that has a similar face as the human face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Eye and Nose Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Process Dog Face\n",
    "Adjust size and orientation so eye and nose location match the human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Feature Melding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Generate Image of Human/Dog Face Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Super Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create a Super Resolution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Apply Super Resolution to the Generated Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Final Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Build Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
